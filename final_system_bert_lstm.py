# -*- coding: utf-8 -*-
"""Final system - bert_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PfCwWCtepMnas31Lkigi6aYgb93dDr8O
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel
import pandas as pd
import numpy as np
import ast

# Load the dataset
data = pd.read_excel('/content/preprocessed_dataset_3.xlsx')
data

# Extract texts and labels after processing all chunks
texts = data['our_processed_text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x).tolist()
labels = data['subreddit'].tolist()

# BERT Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Custom Dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len): # Changed _init_ to __init__
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self): # Changed _len_ to __len__
        return len(self.texts)

    def __getitem__(self, index): # Changed _getitem_ to __getitem__
        text = self.texts[index]
        label = self.labels[index]

        # Check if text is nan and replace it with an empty string
        if pd.isnull(text) or text == np.nan:  # Check for NaN using both pd.isnull and np.nan for different NaN representations
            text = ""  # Replace NaN with empty string to avoid tokenizer error

        # Tokenize the input text using BERT tokenizer
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',  # Updated padding method
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Dataset and DataLoader
MAX_LEN = 128
BATCH_SIZE = 16

# Convert labels to numerical format (you can adjust this mapping accordingly)
label_mapping = {label: idx for idx, label in enumerate(sorted(set(labels)))}
numerical_labels = [label_mapping[label] for label in labels]

# Create Dataset and DataLoader
dataset = TextDataset(texts, numerical_labels, tokenizer, MAX_LEN)
train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Define Model
class BERTLSTMClassifier(nn.Module):
    def __init__(self, bert_model, hidden_size, num_labels): # Changed _init_ to __init__
        super(BERTLSTMClassifier, self).__init__()
        self.bert = bert_model
        self.lstm = nn.LSTM(input_size=768, hidden_size=hidden_size, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, num_labels)  # Bidirectional LSTM

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # Freeze BERT layers
            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        lstm_output, _ = self.lstm(bert_output)
        output = self.fc(lstm_output[:, -1, :])  # Use the last output from LSTM
        return output

# Initialize Model, Loss, Optimizer
bert_model = BertModel.from_pretrained('bert-base-uncased')
model = BERTLSTMClassifier(bert_model, hidden_size=128, num_labels=len(label_mapping))

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

# Training function
def train_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct_predictions = 0

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, preds = torch.max(outputs, dim=1)
        correct_predictions += torch.sum(preds == labels)

    return correct_predictions.double() / len(dataloader.dataset), total_loss / len(dataloader)

# Training loop
num_epochs = 1

for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}')
    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    print(f'Train loss: {train_loss:.4f}, accuracy: {train_acc:.4f}')

# Save the model
torch.save(model.state_dict(), 'bert_lstm_classifier.pth')

def predict_label(text):

    # Handle missing or NaN input text
    if pd.isnull(text) or text == np.nan:
        text = ""

    # Tokenize the input text
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_token_type_ids=False,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt',
    )

    # Move tensors to the appropriate device
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    # Set the model to evaluation mode and make prediction
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids, attention_mask)
        _, pred = torch.max(outputs, dim=1)

    # Convert numerical prediction to label
    inverse_label_mapping = {idx: label for label, idx in label_mapping.items()}
    predicted_label = inverse_label_mapping[pred.item()]

    return predicted_label

original_text = "This is an absolute disaster. Everything is falling apart. I'm completely and utterly screwed."
illness = predict_label(original_text)
print("Predicted Label/ Mental illness:", illness)

pip install -q -U google-generativeai

import google.generativeai as genai
from google.colab import userdata
genai.configure(api_key=userdata.get("gemini_key"))
model_gen = genai.GenerativeModel("gemini-1.5-flash")

def positive_rephrasing(text, illness):
  response = model_gen.generate_content(["Based on the text and illness rephrase the text to be less triggering for the illness and more positive without changing the contextual meaning. Text:", text, "Illness:", illness, "Also in the next line provide Indian helpline number and website for that mental illness."])
  return response.text

rephrased_text = positive_rephrasing(original_text,illness)
print(rephrased_text)